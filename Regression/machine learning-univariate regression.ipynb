{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning \n",
    "# basic concept:\n",
    "\n",
    "-Experience\n",
    "-performance measure\n",
    "-Task\n",
    "\n",
    "1. Supervised learning\n",
    "-already have the right answer of dataset: every sample has a right label\n",
    "Regression problem: continuous answer\n",
    "classification: like 0,1 problem\n",
    "\n",
    "2. Unsupervised learning:\n",
    "clustering algorithm: no hints for the algorithm to how to classify; find the data structure by itself\n",
    "cocktail party algorithm: seperate two or more different data sourses\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate regression: \n",
    "\n",
    "1. Composition\n",
    "training data set--input/features; output/ target \n",
    "hypothesis function h from learging algorithm: x--y\n",
    "Represetation of hypothesis function:\n",
    "H(theta)=theta0+theta1*x\n",
    "\n",
    "2. Cost function:\n",
    "The core task is to estimate the value of theta0 and theta1 by minimizing differnece between h() and y\n",
    "求差值的平方之和\n",
    "the function needed to be minimized is the cost function\n",
    "Squared error function--the most common regression function\n",
    "-using contour graph to represent cost function to find the optimal point when considering two or more parameters\n",
    "-what we want to do is to find an efficient algorithm to obtain the minimum J cost function value\n",
    "\n",
    "3. Gradient descent: method to approach the minimum value\n",
    "like the methon in WRM course\n",
    "the problem with local optimum\n",
    "random starting point \n",
    "Gradient descent equation:\n",
    "parameter: learning rate--determines how much is one step \n",
    " \n",
    "# Simutaneous Update !! \n",
    "-Calculate the partial derivative part first and then the two theta parameters:\n",
    "-the partial derivative is for theta\n",
    "-first calculate both theta0 and theta1 and then update them at the same time\n",
    "![picture1](img/Screenshot_1.jpg)\n",
    "\n",
    "Important to choose a proper learning rate!!\n",
    "-local optima problem: \n",
    "-the gradient will decrease smaller and smaller as approching to local optima point as the decrease of derivative value automatically\n",
    "\n",
    "4. Linear regression algorithm\n",
    "![picture2](img/Screenshot_2.jpg)\n",
    "The derivative of the partial function for theta1 and theta0---is the slope of cost function J\n",
    "![3](img/3.jpg)\n",
    "\n",
    "# How it works\n",
    "-Batch gradient descent algorithm:\n",
    "for each step we looking at all the training examples and we calculate the sum over m training samples\n",
    "\n",
    "# Matrix calculation\n",
    "![4](img/4.jpg)\n",
    "Properties:\n",
    "-Identity matrix I: all value on diagonal are 1\n",
    "I*A =A*I\n",
    "\n",
    "-Matrix inverse\n",
    "A(-1)*A=I\n",
    "A should be a matrix without 0 and should be a square matrix\n",
    "Matrix without an inverse matrix called singular matrix\n",
    "\n",
    "-Matrix transpose:\n",
    "exchange of column and row: n*m matrix to m*n matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Regression\n",
    "\n",
    "1. Multi features: (多变量)\n",
    "-Notations:\n",
    "![6](img/6.jpg)\n",
    "X(2)--the second training sample among whole training set--training sample is a vector including all the features--called feature vector\n",
    "X(2)_3--the 3rd feature of the sample(one of the variate)\n",
    "\n",
    "-New form of hypothesis function:\n",
    "![7](img/7.jpg)\n",
    "Feature vector has n+1 features with x0=1\n",
    "parameter vector transpose * feature vector = hypothesis function\n",
    "![8](img/8.jpg)\n",
    "\n",
    "2. Multi gradient descent\n",
    "![9](img/9.jpg)\n",
    "Only updated notations\n",
    "\n",
    "-Practical use：\n",
    "Feature scaling: make sure different features in the similar ranges to make it convert quickly\n",
    "(不同的特征值的取值范围不同导致theta梯度下降所需要的时间更长)\n",
    "Therefore it is necessary to normalize the feature value by dividing the maximum number(0-1)\n",
    "![10](img/10.jpg)\n",
    "\n",
    "- Learning rate:\n",
    "-- make sure gradient descent works properly: Plotting the J figure !\n",
    "![11](img/11.jpg)\n",
    "J(theta)should be decreased at every step and also at the tail part we can judge converged or not to ensure the number of converges\n",
    "Some convergence test: When J() decrease less than a small value in one iteration then it is converged\n",
    "\n",
    "how to choose the correct learning rate:\n",
    "based on the J decrease graph as a function of iterations\n",
    "and then try learning rate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features and Polynominal regression for non-linear function\n",
    "\n",
    "When linear model is not a good fit, try other polynominal regression model\n",
    "How to fit a non-linear problem:\n",
    "set features with different orders and then apply the same way as linear regression----size scaling is more important\n",
    "![12](img/12.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal equation--better way to solve linear regression problem in some cases\n",
    "\n",
    "- Method to solve parameter analytically\n",
    "![13](img/13.jpg)\n",
    "First step: Matrix x(i)->X(T) transpose the column vector to row vector\n",
    "Then get the normal equation to represent theta!---the equation can get the value of theta to achive the minimum J\n",
    "- Difference bewteen normal equation and gradient descent\n",
    "![14](img/14.jpg)\n",
    "\n",
    "- Normal equation and its non-inversibility\n",
    "When Xt$*$X is not reversible:\n",
    "Two Reasons:\n",
    "![15](img/15.jpg)\n",
    "Solutions:\n",
    "-reduce the redundant features(especially those dependent on others)\n",
    "-do regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
